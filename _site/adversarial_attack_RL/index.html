<!DOCTYPE html>
<html lang="en">
  <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="user-scalable=no" />
      <meta name="description" content="
Deep Reinforcement Learning (RL) agents have been shown to be vulnerable to adversarial attacks Huang et al. (2017). However, no tactic other than attacking at every time steps has been explored. In this work, we introduced two novel tactics for adversarial attacks on deep RL agents (i.e., parsimonious attack and enchanting attack). For parsimonious attack, we aim at minimizing the reward while limiting the number of attacks to prevent the attack to be spotted. We propose a novel method to selectively force the least likely action only on a limited number of time steps. For enchanting attack, we aim at luring the agent to a designated target state specified by the adversary. We propose to combine a generative model for predicting the future states and a planning algorithm to launch a sequence of adversarial attacks. In 5 Atari games, our proposed methods significantly reduce reward on both DQN and A3C agents by attacking very few frames (on average 25% time steps) and luring deep RL agent toward target states approximately 40- steps in the future in some games with ∼ 70% success rate.
      " />
      <title>Adversarial Attack RL </title>
      <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" />
      <link rel="stylesheet" href="css/base.css" />
      <meta property="og:url" content="http://yclin.me/adversarial_attack_RL/" /> 
      <meta property="og:type" content="website"/>
      <meta property="og:title" content="Adversarial Attack RL" />
      <meta property="og:site_name" content="Adversarial Attack RL"/>
      <meta property="og:description" content="
Deep Reinforcement Learning (RL) agents have been shown to be vulnerable to adversarial attacks Huang et al. (2017). However, no tactic other than attacking at every time steps has been explored. In this work, we introduced two novel tactics for adversarial attacks on deep RL agents (i.e., parsimonious attack and enchanting attack). For parsimonious attack, we aim at minimizing the reward while limiting the number of attacks to prevent the attack to be spotted. We propose a novel method to selectively force the least likely action only on a limited number of time steps. For enchanting attack, we aim at luring the agent to a designated target state specified by the adversary. We propose to combine a generative model for predicting the future states and a planning algorithm to launch a sequence of adversarial attacks. In 5 Atari games, our proposed methods significantly reduce reward on both DQN and A3C agents by attacking very few frames (on average 25% time steps) and luring deep RL agent toward target states approximately 40- steps in the future in some games with ∼ 70% success rate.
      "/>
      <meta property="og:image" content=""/>
  </head>
  <body>
  <div id="root"></div>
  <script src="https://code.jquery.com/jquery-3.1.0.min.js"   integrity="sha256-cCueBR6CsyA4/9szpPfrX3s49M9vUU5BgtiJj06wt/s="   crossorigin="anonymous"></script>
  <script src="js/polyfill.js"></script>
  <script src="js/bundle.js"></script>
  </body>
</html>
